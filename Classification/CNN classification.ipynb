{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "from PIL import Image, ImageStat\n",
    "import sys\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(dir):\n",
    "    file_list=listdir(dir)\n",
    "    for filename in file_list:\n",
    "        print (filename)\n",
    "        if \"panorama\" in filename and \"cropped\" not in filename:\n",
    "            path=\"\"\n",
    "            path=dir+filename\n",
    "            im=Image.open(path)\n",
    "            out=im.resize((104,208),Image.ANTIALIAS)\n",
    "            print (\"%s has been resized\"%filename)\n",
    "            out.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(image_path):\n",
    "    \"\"\"\n",
    "    Get a numpy 2D array of an image so that one can access RGBA[x][y] or RGBA[x,y].\n",
    "    \n",
    "    Also get a cropped image for further use for hsv and k-means clustering\n",
    "    \n",
    "    \"\"\"\n",
    "    origin = Image.open(image_path, 'r')\n",
    "    width, height = origin.size\n",
    "    area = (0, 0, width, 0.5*height)\n",
    "    image = origin.crop(area) # crop top half of the image\n",
    "#     result = Image.fromarray(image.astype(np.uint8))\n",
    "    image.save(file+\"_cropped.png\")\n",
    "    width, height = image.size\n",
    "#     print (image.size)\n",
    "    pixel_values = list(image.getdata())\n",
    "    if image.mode == 'RGBA':\n",
    "        channels = 4\n",
    "    elif image.mode == 'L':\n",
    "        channels = 1\n",
    "    else:\n",
    "        print(\"Unknown mode: %s\" % image.mode)\n",
    "        return None\n",
    "#     pixel_values = np.array(pixel_values).reshape((width, height, channels))\n",
    "    pixel_values = np.array(pixel_values).reshape((height, width, channels))\n",
    "#     print (np.shape(pixel_values))\n",
    "\n",
    "    return pixel_values,image,origin #return pixels matrix with RGBA value, and cropped half image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bri(pixels):\n",
    "    \"\"\"\n",
    "    Get a numpy 2D array of an image so that one can access brightness[x][y]\n",
    "       \n",
    "    \"\"\"\n",
    "    brightness=[[0 for x in range(len(pixels[0]))] for y in range(len(pixels))]\n",
    "    # Matrix = [[0 for x in range(w)] for y in range(h)]\n",
    "    for i in range(len(pixels)):\n",
    "        for j in range(len(pixels[0])):\n",
    "            R,G,B,A=pixels[i,j]\n",
    "            brightness[i][j]=(0.2126*R + 0.7152*G + 0.0722*B)/255\n",
    "    return brightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blackC-1010_-37.808706898558945_144.98444713162672_panorama.png_cropped.png\n",
      "bigS-1060_-37.80868577546702_144.9850156495497_panorama.png\n",
      "bigS-1060_-37.80868577546702_144.9850156495497_panorama.png has been resized\n",
      "blackC-1010_-37.808706898558945_144.98444713162672_panorama.png\n",
      "blackC-1010_-37.808706898558945_144.98444713162672_panorama.png has been resized\n",
      "whiteB-0100_-37.80909086146474_144.9741000486449_panorama.png\n",
      "whiteB-0100_-37.80909086146474_144.9741000486449_panorama.png has been resized\n",
      "sky-1270_-37.80859702864577_144.98740342127365_panorama.png_cropped.png\n",
      "sky-1810_-37.80836860123093_144.99354337933758_panorama.png\n",
      "sky-1810_-37.80836860123093_144.99354337933758_panorama.png has been resized\n",
      "bigS-2290_-37.80816528711204_144.99900108792806_panorama.png\n",
      "bigS-2290_-37.80816528711204_144.99900108792806_panorama.png has been resized\n",
      "tree-0110_-37.809086647010126_144.97421375343805_panorama.png_cropped.png\n",
      "MNIST_data\n",
      "buliding-0140_-37.80907400299053_144.9745548677395_panorama.png_cropped.png\n",
      "bigS-0790_-37.80879980770527_144.98194564890238_panorama.png\n",
      "bigS-0790_-37.80879980770527_144.98194564890238_panorama.png has been resized\n",
      "sky-1810_-37.80836860123093_144.99354337933758_panorama.png_cropped.png\n",
      ".DS_Store\n",
      "whiteB-0670_-37.80885046312688_144.98058120112736_panorama.png_cropped.png\n",
      "whiteB-0300_-37.80900655160748_144.9763741420407_panorama.png_cropped.png\n",
      "tree-0110_-37.809086647010126_144.97421375343805_panorama.png\n",
      "tree-0110_-37.809086647010126_144.97421375343805_panorama.png has been resized\n",
      "blackC-0970_-37.80872379506533_144.98399231705417_panorama.png_cropped.png\n",
      "buliding-1760_-37.808389765304874_144.99297486629814_panorama.png_cropped.png\n",
      "tree-2310_-37.808156810226265_144.99922849180032_panorama.png\n",
      "tree-2310_-37.808156810226265_144.99922849180032_panorama.png has been resized\n",
      "sky-2420_-37.80811017954074_145.00047921216438_panorama.png\n",
      "sky-2420_-37.80811017954074_145.00047921216438_panorama.png has been resized\n",
      "sky-1360_-37.808558979540216_144.98842675025523_panorama.png\n",
      "sky-1360_-37.808558979540216_144.98842675025523_panorama.png has been resized\n",
      "bigS-0790_-37.80879980770527_144.98194564890238_panorama.png_cropped.png\n",
      "buliding-0380_-37.80897281542424_144.97728377794476_panorama.png_cropped.png\n",
      "tree-2310_-37.808156810226265_144.99922849180032_panorama.png_cropped.png\n",
      "blackC-2520_-37.8080677765338_145.00161622930622_panorama.png_cropped.png\n",
      "blackC-0970_-37.80872379506533_144.98399231705417_panorama.png\n",
      "blackC-0970_-37.80872379506533_144.98399231705417_panorama.png has been resized\n",
      "tree-0040_-37.80911614589736_144.9734178196137_panorama.png\n",
      "tree-0040_-37.80911614589736_144.9734178196137_panorama.png has been resized\n",
      "whiteB-1140_-37.80865197283703_144.98592527754985_panorama.png_cropped.png\n",
      "buliding-1760_-37.808389765304874_144.99297486629814_panorama.png\n",
      "buliding-1760_-37.808389765304874_144.99297486629814_panorama.png has been resized\n",
      "buliding-0140_-37.80907400299053_144.9745548677395_panorama.png\n",
      "buliding-0140_-37.80907400299053_144.9745548677395_panorama.png has been resized\n",
      "whiteB-0300_-37.80900655160748_144.9763741420407_panorama.png\n",
      "whiteB-0300_-37.80900655160748_144.9763741420407_panorama.png has been resized\n",
      "tree-0900_-37.80875335974397_144.9831963910514_panorama.png\n",
      "tree-0900_-37.80875335974397_144.9831963910514_panorama.png has been resized\n",
      "tree-0920_-37.80874491323935_144.9834237985458_panorama.png_cropped.png\n",
      "blackC-1630_-37.80844477911086_144.99149673087072_panorama.png\n",
      "blackC-1630_-37.80844477911086_144.99149673087072_panorama.png has been resized\n",
      "bigS-2290_-37.80816528711204_144.99900108792806_panorama.png_cropped.png\n",
      "bigS-0860_-37.80877025144173_144.98274157590654_panorama.png_cropped.png\n",
      "blackC-2520_-37.8080677765338_145.00161622930622_panorama.png\n",
      "blackC-2520_-37.8080677765338_145.00161622930622_panorama.png has been resized\n",
      "bigS-0820_-37.80878714139091_144.98228676055362_panorama.png_cropped.png\n",
      "tree-0920_-37.80874491323935_144.9834237985458_panorama.png\n",
      "tree-0920_-37.80874491323935_144.9834237985458_panorama.png has been resized\n",
      "tree-0040_-37.80911614589736_144.9734178196137_panorama.png_cropped.png\n",
      "sky-1100_-37.80866887502632_144.98547046365388_panorama.png\n",
      "sky-1100_-37.80866887502632_144.98547046365388_panorama.png has been resized\n",
      "sky-1270_-37.80859702864577_144.98740342127365_panorama.png\n",
      "sky-1270_-37.80859702864577_144.98740342127365_panorama.png has been resized\n",
      "classification.ipynb\n",
      "sky-2420_-37.80811017954074_145.00047921216438_panorama.png_cropped.png\n",
      "whiteB-0030_-37.80912035958696_144.97330411472973_panorama.png_cropped.png\n",
      "bigS-0820_-37.80878714139091_144.98228676055362_panorama.png\n",
      "bigS-0820_-37.80878714139091_144.98228676055362_panorama.png has been resized\n",
      "sky-1100_-37.80866887502632_144.98547046365388_panorama.png_cropped.png\n",
      "sky-1360_-37.808558979540216_144.98842675025523_panorama.png_cropped.png\n",
      "blackC-0850_-37.808774474092964_144.98262787208782_panorama.png\n",
      "blackC-0850_-37.808774474092964_144.98262787208782_panorama.png has been resized\n",
      ".ipynb_checkpoints\n",
      "tree-0900_-37.80875335974397_144.9831963910514_panorama.png_cropped.png\n",
      "bigS-1060_-37.80868577546702_144.9850156495497_panorama.png_cropped.png\n",
      "buliding-0380_-37.80897281542424_144.97728377794476_panorama.png\n",
      "buliding-0380_-37.80897281542424_144.97728377794476_panorama.png has been resized\n",
      "whiteB-0030_-37.80912035958696_144.97330411472973_panorama.png\n",
      "whiteB-0030_-37.80912035958696_144.97330411472973_panorama.png has been resized\n",
      "blackC-0850_-37.808774474092964_144.98262787208782_panorama.png_cropped.png\n",
      "bigS-0860_-37.80877025144173_144.98274157590654_panorama.png\n",
      "bigS-0860_-37.80877025144173_144.98274157590654_panorama.png has been resized\n",
      "whiteB-1140_-37.80865197283703_144.98592527754985_panorama.png\n",
      "whiteB-1140_-37.80865197283703_144.98592527754985_panorama.png has been resized\n",
      "whiteB-0670_-37.80885046312688_144.98058120112736_panorama.png\n",
      "whiteB-0670_-37.80885046312688_144.98058120112736_panorama.png has been resized\n",
      "blackC-1630_-37.80844477911086_144.99149673087072_panorama.png_cropped.png\n",
      "whiteB-0100_-37.80909086146474_144.9741000486449_panorama.png_cropped.png\n"
     ]
    }
   ],
   "source": [
    "path1 = \"/Users/yimingqiu/Desktop/class/\"\n",
    "files= listdir(path1)\n",
    "\n",
    "convert(path1)\n",
    "# Sgray=0.0\n",
    "# for K in range(4, 16):\n",
    "# while Sgray<=1:\n",
    "#     true_po_list=[]\n",
    "#     precision_list=[]\n",
    "#     edge_list=[]\n",
    "    # print (files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigS-0790_-37.80879980770527_144.98194564890238_panorama.png (1, 43264)\n",
      "bigS-0820_-37.80878714139091_144.98228676055362_panorama.png (2, 43264)\n",
      "bigS-0860_-37.80877025144173_144.98274157590654_panorama.png (3, 43264)\n",
      "bigS-1060_-37.80868577546702_144.9850156495497_panorama.png (4, 43264)\n",
      "bigS-2290_-37.80816528711204_144.99900108792806_panorama.png (5, 43264)\n",
      "blackC-0850_-37.808774474092964_144.98262787208782_panorama.png (6, 43264)\n",
      "blackC-0970_-37.80872379506533_144.98399231705417_panorama.png (7, 43264)\n",
      "blackC-1010_-37.808706898558945_144.98444713162672_panorama.png (8, 43264)\n",
      "blackC-1630_-37.80844477911086_144.99149673087072_panorama.png (9, 43264)\n",
      "blackC-2520_-37.8080677765338_145.00161622930622_panorama.png (10, 43264)\n",
      "buliding-0140_-37.80907400299053_144.9745548677395_panorama.png (11, 43264)\n",
      "buliding-0380_-37.80897281542424_144.97728377794476_panorama.png (12, 43264)\n",
      "buliding-1760_-37.808389765304874_144.99297486629814_panorama.png (13, 43264)\n",
      "sky-1100_-37.80866887502632_144.98547046365388_panorama.png (14, 43264)\n",
      "sky-1270_-37.80859702864577_144.98740342127365_panorama.png (15, 43264)\n",
      "sky-1360_-37.808558979540216_144.98842675025523_panorama.png (16, 43264)\n",
      "sky-1810_-37.80836860123093_144.99354337933758_panorama.png (17, 43264)\n",
      "sky-2420_-37.80811017954074_145.00047921216438_panorama.png (18, 43264)\n",
      "tree-0040_-37.80911614589736_144.9734178196137_panorama.png (19, 43264)\n",
      "tree-0110_-37.809086647010126_144.97421375343805_panorama.png (20, 43264)\n",
      "tree-0900_-37.80875335974397_144.9831963910514_panorama.png (21, 43264)\n",
      "tree-0920_-37.80874491323935_144.9834237985458_panorama.png (22, 43264)\n",
      "tree-2310_-37.808156810226265_144.99922849180032_panorama.png (23, 43264)\n",
      "whiteB-0030_-37.80912035958696_144.97330411472973_panorama.png (24, 43264)\n",
      "whiteB-0100_-37.80909086146474_144.9741000486449_panorama.png (25, 43264)\n",
      "whiteB-0300_-37.80900655160748_144.9763741420407_panorama.png (26, 43264)\n",
      "whiteB-0670_-37.80885046312688_144.98058120112736_panorama.png (27, 43264)\n",
      "whiteB-1140_-37.80865197283703_144.98592527754985_panorama.png (28, 43264)\n",
      "[0.6101803921568626, 0.6101803921568626, 0.6101803921568626, 0.6101803921568626, 0.6101803921568626, 0.6101803921568626, 0.6062588235294116, 0.6062588235294116, 0.6152392156862745, 0.6144054901960785, 0.6113176470588235, 0.6113176470588235, 0.6113176470588235, 0.6113176470588235, 0.6113176470588235, 0.6113176470588235, 0.6107309803921568, 0.6107309803921568, 0.6107309803921568, 0.6107309803921568, 0.6107309803921568, 0.6107309803921568, 0.6107309803921568, 0.6107309803921568, 0.6082094117647059, 0.6082094117647059, 0.6082094117647059, 0.6082094117647059, 0.6082094117647059, 0.6082094117647059, 0.6082094117647059, 0.6082094117647059, 0.6082094117647059, 0.6082094117647059, 0.6082094117647059, 0.6082094117647059, 0.6121309803921569, 0.6121309803921569, 0.6121309803921569, 0.6121309803921569, 0.6115647058823529, 0.6118478431372548, 0.6118478431372548, 0.6146525490196079, 0.6079262745098039, 0.6107309803921568, 0.6079262745098039, 0.6098972549019607, 0.6098972549019607, 0.6098972549019607, 0.6098972549019607, 0.6098972549019607, 0.6107309803921568, 0.6107309803921568, 0.6107309803921568, 0.6107309803921568, 0.6082094117647059, 0.6082094117647059, 0.6082094117647059, 0.6082094117647059, 0.6042878431372549, 0.6042878431372549, 0.6082094117647059, 0.6073756862745098, 0.6107309803921568, 0.6107309803921568, 0.6107309803921568, 0.6107309803921568, 0.6068094117647059, 0.6068094117647059, 0.6070925490196077, 0.6107309803921568, 0.6101803921568626, 0.6101803921568626, 0.6101803921568626, 0.6101803921568626, 0.6101803921568626, 0.6101803921568626, 0.6101803921568626, 0.6101803921568626, 0.6121309803921569, 0.6121309803921569, 0.6121309803921569, 0.6121309803921569, 0.612964705882353, 0.612964705882353, 0.612964705882353, 0.612964705882353, 0.6160525490196077, 0.6160525490196077, 0.6160525490196077, 0.6160525490196077, 0.6160525490196077, 0.6160525490196077, 0.6160525490196077, 0.6160525490196077, 0.6160525490196077, 0.6160525490196077, 0.6160525490196077, 0.6160525490196077]\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "train_x = []\n",
    "for file in sorted(files):\n",
    "    if \"panorama\" in file and \"cropped\" not in file:\n",
    "#         file=path1+file\n",
    "        pixels,image,origin=get_image(file)\n",
    "#         print (np.shape(pixels))\n",
    "            # print (pixel1)\n",
    "        brightness=get_bri(pixels)\n",
    "        \n",
    "#             edge_cal(file)\n",
    "        brightness[i]=np.asarray(brightness)\n",
    "        brightness[i]=brightness[i].reshape(-1)\n",
    "        train_x.append(brightness[i])\n",
    "        print(file,np.shape(train_x))\n",
    "        i+=1\n",
    "#         mark_sky(pixels,label2,origin,file,Sgray)\n",
    "print(brightness[0][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 6)\n"
     ]
    }
   ],
   "source": [
    "train_y = []\n",
    "for i in range(5):\n",
    "    train_y.append([1,0,0,0,0,0])\n",
    "for i in range(5):\n",
    "    train_y.append([0,1,0,0,0,0])\n",
    "for i in range(3):\n",
    "    train_y.append([0,0,1,0,0,0])\n",
    "for i in range(5):\n",
    "    train_y.append([0,0,0,1,0,0])\n",
    "for i in range(5):\n",
    "    train_y.append([0,0,0,0,1,0])\n",
    "for i in range(5):\n",
    "    train_y.append([0,0,0,0,0,1])\n",
    "\n",
    "print(np.shape(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27, 43264) (27, 6)\n"
     ]
    }
   ],
   "source": [
    "test_x = train_x[0]\n",
    "test_y = train_y[0]\n",
    "train_x=np.delete(train_x,1,0)\n",
    "train_y=np.delete(train_y,1,0)\n",
    "print(np.shape(train_x),np.shape(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 43264)\n"
     ]
    }
   ],
   "source": [
    "test_x=test_x.reshape((1,-1))\n",
    "print(np.shape(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 6)\n"
     ]
    }
   ],
   "source": [
    "test_y= np.array(test_y)\n",
    "test_y=test_y.reshape((1,-1))\n",
    "print(np.shape(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init weight\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev = 0.1) #normal distribution\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init bias\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x,W):\n",
    "#     x input tensor of shape [batch, in_height, in_width, in_channels]\n",
    "# W filter/kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "# strides[0]=strides[3]=1 strides[x] stands \"x\"direction, strides[y]stands \"y\" dicrection\n",
    "# padding: \"SAME\",OR \"VALID\"\n",
    "    return tf.nn.conv2d(x,W,strides=[1,1,1,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_pool_2x2(x):\n",
    "#     ksize [1,x,y,1]\n",
    "    return tf.nn.max_pool(x,ksize=[1,2,2,1],strides=[1,2,2,1],padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 100\n",
    "\n",
    "# n_batch = mnist.train.num_examples // batch_size\n",
    "\n",
    "batch_size=1\n",
    "\n",
    "#placeholder\n",
    "x = tf.placeholder(tf.float32,[None,43264])\n",
    "y = tf.placeholder(tf.float32,[None,6])\n",
    "\n",
    "# change X format into a 4D vector [baatch, in_height, in_width, in_channels]\n",
    "x_image = tf.reshape(x,[-1,28,28,3])\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "# init first Convolutional Layer\n",
    "W_conv1 = weight_variable([5,5,3,32]) #5*5 window, 32 kernals extract feature from 1 \n",
    "b_conv1 = bias_variable([32]) #each kernal 1 bias\n",
    "\n",
    "# Convolution using relu activation function\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image,W_conv1) +b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)  #max-pooling\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "# init second Convolutional Layer\n",
    "W_conv2 = weight_variable([5,5,32,64]) #5*5 window, 64 kernals extract feature from 1 \n",
    "b_conv2 = bias_variable([64]) #each kernal 1 bias\n",
    "\n",
    "# Convolution using relu activation function\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1,W_conv2) +b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)  #max-pooling\n",
    "\n",
    "# 104*104 image → 52*52 after first pooling\n",
    "# 52*52 image → 26*26 after second pooling\n",
    "# get 64*26*26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init FC layer\n",
    "W_fc1 = weight_variable([64*26*26,1024]) #last layer 64*26*26 neurons\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "# conver output of second pooling layer into 1 demonsion\n",
    "h_pool2_flat= tf.reshape(h_hool2, [-1,64*26*26])\n",
    "\n",
    "# first FC layer output\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat,W_fc1)+b_fc1)\n",
    "\n",
    "temp_prob = tf.placeholder(tf.float32)\n",
    "h_hc1_drop = tf.nn.dropout(h_fc1,keep_prob)\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "# second FC layer\n",
    "W_fc2 = weight_variable([1024,6])\n",
    "b_fc2 = bias_variable([6])\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "# output\n",
    "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop,W_fc2)+b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y,logits = prediction))\n",
    "\n",
    "# AdamOptimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# store into a boolean list\n",
    "correct_prediction=  tf.equal(tf.largmax(prediction,1),tf.argmax(y,1))#argmax return the position of the max number\n",
    "\n",
    "# get accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(21):\n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys =  train_x,train_y\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n",
    "        \n",
    "        acc = sess.run(accuracy,feed_dict={x:test_x,y:test_y, keep_prob:1.0 })\n",
    "        print(\"Iter \" + str(epoch) + \",Testing Accuracy \" + str(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
